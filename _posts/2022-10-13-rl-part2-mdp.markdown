---
layout: post
title:  Reinforcement Learning Part 2 - Markov Decision Process (MDP)
date: 2022-10-13 6:00:00
description: A detailed description of markov decision process which serves as mathematical foundation of formulating RL problems.
tags: RL
categories: RL
---

Markov decision processes (MDPs) serve as a mathematical formalism to describe sequential decision making problems. It is used to formally define environments in reinforcement learning. The agent, which is the component that takes the decisions, interacts with this environment to learn a desired behavior in a trial and error manner. Note that the terms MDP and environment are often used interchangeably. Here, the term MDP is used when describing the formal properties, and the term environment when referring to implementation aspects of reinforcement learning. 

# Elements of a Markov Decision Process

In the following, the components which are used to define the MDP are listed and described in more detail.

## State Space
The _state space_ defines the possible states that the MDP can possibly evolve into. A state is used to describe the MDP at a given time step. The agent uses a state as decision criterion for its choice of action (with the goal to maximize rewards). 

Let's take chess as an example: One state would contain information about every single piece on the board. Based on this state, the agent is given the possibility to plan its next move (deciding which piece is moved to which field). This state could be either represented numerically or by an image of the chess board (which would make the overall task harder as it adds further complexity of image processing). In this example, the state space contains every possible (legal) positioning of chess pieces (according to the "Shannon number", the state space of chess contains around $$10^{120}$$ different states).

#### Markov Property
A fundamental concept of states in MDPs is the _Markov property_, which defines that “The future is independent of the past given the present”, which can be formally described by:

$$P[S_{t+1}|S_t] = P[S_{t+1}|S_1, S_2..., S_t].$$

This means, in order to not violate the Markov Property, a state must “include information about all aspects of the past agent–environment interaction that make a difference for the future”(<cite>R. S. Sutton and A. G. Barto</cite>). In the end, the agent takes its decision based on the state of the current time step and not on states it has seen before (as if the agent had no memory).

Let's take table football as an example:
Imagine the scenario where we want to train a table football goalkeeper to hold a ball. In this scenario, we define an image of the current game state as our state. Meaning, a state would look like this (Note that the ball is shown in orange):  
{% include figure.html path="assets/img/posts/markov_property.png" class="img-fluid center rounded z-depth-1" width="300"%}
Could you conclude from only this picture in which direction the goalkeeper should move? Probably not. The information about the direction and velocity of the ball is missing and can only be estimated from several consecutive images. This means, in order to follow the Markov property we must include those past states (which are images in our exemplary scenario) and therefore give the agent the possibility to reason about the direction and velocity of the ball.

#### Start and Terminal States

There are two types of special states. First, there are _initial states_ which are often denoted as $$S_0$$. Second, there are _terminal states_ which mark the end of an episode. Terminal states are usually denoted with a capital T, like $$S_T$$. An interaction between environment and agent that stretches from $$S_0, A_0, S_1, A_1..., S_T , A_T$$ is called an _episode_. Note that not all MDPs have terminal states. Those that have terminal states are often referred to as _episodic MDPs_, whereas those without any terminal states are referred to as _continuing MDPs_.

<!-- ToDo: Beispiel was eine episode und Terminal state ist und was so die Design decisions sind... -->

## Action Space

The _action space_ defines the possible Actions $$A_t$$ the agent can take in a given state. Taking an action triggers a state transition and results in a new MDP state $$S_{t+1}$$. Action spaces can be categorized by their action type, which can be either discrete or continuous. In continuous action spaces, an action can take any real number in a specified interval. On the other hand, the actions in discrete action spaces can only take discrete values.

Let's take our table football example again where we want to train an agent to control the goalkeeper: When using a discrete action space, the available actions could be left, right, and "do nothing". Left and right could move the goalkeeper to one of the sides by a fixed distance. In a continuous action space, the available actions would lie in an interval, which defines the distance and direction (e.g. negative: left, positive: right) the goalkeeper would move. In this example, choosing a continuous action space would offer greater control precision.

## Reward Function 

After taking an action, the agent not only receives information about the new MDP state but also a scalar reward $$R_{t+1} \in \mathbb{R}$$. The reward serves as feedback on the quality of a certain action in a given state. The goal of the agent is to maximize the sum rewards over time (as we will see in a minute, this sum gets _discounted_). The calculation of the reward can be expressed as a function $$r(s, a, s^\prime)$$, which depends on the current state $$s$$, the successor state $$s^\prime$$ as well as the action $$a$$ taken to transfer from the current to the successor state. The reward function is part of the implementation of every environment and is very important as it encodes the goal of the task at hand. Choosing the reward definition wisely is key to the functionality of reinforcement learning.

The reward given to the agent can be categorized as either being _sparse_ or _dense_. _Sparse rewards_ are only given to the agent rarely (typically at the end of an episode). In contrast, _dense rewards_ are given to the agent often (typically after every action). Let's take our table football goalkeeper example again: In a _sparse reward_ scenario, the agent would only receive a reward after the goalkeeper holds the ball (e.g. r=1) or a goal was scored (e.g. r=-1). In a _dense reward_ scenario, the agent could receive a reward after every action in form of the distance between the goalkeeper and the ball (the lower the distance, the higher the reward). This way, the agent could easily learn the behavior of moving close to the ball in order to hold it. Most of the times, using a _dense reward_ makes learning easier, as we give the agent feedback more often. On the other hand, sometimes _dense rewards_ are hard to define (for example at chess). Lastly, by using _sparse rewards_ we are indirectly providing prior knowledge to the agent (which we usually don´t want). Like in the goalkeeper scenario, we are providing the agent with the information that it is good to be close to the ball in order to hold it (wouldn´t it be nice, if the agent could pick this behavior up itself?).

Another important concept related to the reward definition is _discounting_. With discounting we regulate the impact of time distant rewards. As an example: If we want to create a trading agent, we might want to define, that money now is worth more than money in two years. Therefore, we _discount_ the rewards which might occur somewhere in the future.
 
This concept is taken up by the _return_ $$G_t$$, which defines the sum of _discounted_ future rewards from the current time step $$t$$. It is defined as follows: 

$$
 \begin{align}
 G_t &= \sum\limits_{i=t+1}^\infty \gamma^{i-t+1} R_i \label{eq:return} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...  \\
 &= R_{t+1}+ \gamma G_{t+1}. && \label{eq:return_recursive}
 \end{align}
$$


The sum is discounted by factor $$\gamma \in [0,1]$$ which is used to define how much impact future rewards have compared to immediate rewards. The higher  $$\gamma$$, the more impact future rewards have on the return. In the case where $$\gamma = 1$$, rewards in the far future would have the same impact on the return as immediate rewards. If $$\gamma = 0$$, we don't care about any future rewards at all. The agent would learn a policy that only focuses on selecting an action that maximizes rewards in the present and ignores all future consequences this might cause (like eating a whole bag of gummy bears now because it is so delicious but completely ignoring the fact that this might cause stomach ache later). The right choice of $$\gamma$$ depends on the problem at hand and is one of the most important hyperparameters in reinforcement learning.

## Transition Function

As mentioned before, taking an action causes the MDP to transfer into a new state. This transition can be described more formally by:

$$p(s^\prime, r|a,s) = P[S_{t+1}=s^\prime, R_t=r| A_{t}=a,S_{t}=s]$$

The transition function $$p$$ defines the probability of arriving at the successor state $$s^\prime$$ and get reward $$r$$, if the agent chooses action $$a$$ while being in state $$s$$. Or to put it in other words: $$p$$ defines the inner dynamics of the MDP.

The transition function makes the environment either _stochastic_ or _deterministic_. In _stochastic environments_, random
dynamics are involved during state transitions. Meaning, given the same state $$s$$ and action $$a$$, the MDP can transition into a set of different possible successor states and experience a different reward.

In contrast, in _deterministic environments_, no random dynamics are involved during state transition. This leads to the fact that the transition function exclusively takes the value 0 or 1. This means, when you take action $$a$$ while being in state $$s$$, you can always expect the same successor state $$s^\prime$$ and $$reward$$.
<!-- ToDo: (like if the agent had full control of the environment by choosing actions, without any other random circumstances happening - which is seldom the case) Unlike the Actions, Reward and State, the transition function is different as it is usually given by the problem at hand. ... -->

<!-- # Examples of MDP definitions -->

# The Agent Side of a MDP

The components described above defined the MDP, which is the formal definition of our environment in reinforcement learning. The other side of the equation is the agent, which takes decisions by choosing actions. Mostly we are interested in finding (or training) an agent that chooses (good) actions which maximize rewards. An agent chooses those actions based on a _policy_, which is denoted as $$\pi$$. More formally, $$\pi$$ specifies the behavior of an agent by defining a probability distribution of actions based on the current state:

$$\pi(a|s) = P[A_{t}=a| S_{t}=s].$$

When talking about _deep_ reinforcement learning, the policy $$\pi$$ is represented by a _deep_ neural network. 

<!-- # A policy, which acts optimally for a given MDP is denoted as _optimal policy_ $$\pi^*$$. -->

To be able to find policies that perform well, we need a way to evaluate them. Therefore, value functions are used in order to assess "how good it is for the agent to be in a given state"(<cite>R. S. Sutton and A. G. Barto</cite>). This is done by the so called _state-value function_ $$V_{\pi}(s)$$. It defines the expected future return when being in state $$s$$ and following policy $$\pi$$ from there on: 

$$V_{\pi}(s) =  E_{\pi}[G_{t} | S_{t} = s].$$

Derived from the state-value function, the _action-value_ function $$Q_{\pi}(s, a)$$ estimates the return when taking an action in a particular state and following policy $$\pi$$ afterward:

$$ Q_{\pi}(s, a) =  E_{\pi}[G_{t} | S_{t} = s, A_{t} = a].$$

The action-values are commonly named _Q-values_. If assumed to act optimally after taking action $$a$$, the action-value function is denoted as $$Q_{*}(s, a)$$.

In case the optimal action-value function $$Q_{*}(s, a)$$ is known, acting optimally can be done by simply following a greedy policy, which chooses the action which maximizes $$Q_{*}(s, a)$$ in each time step: 

$$\pi^*= \underset{a}{\arg\max}\:Q_{*}(s,a).$$

Or in simple words, acting optimally can be done by choosing the best possible action in a given state. The best possible action is the one with the highest Q-value. Anyhow, the assumption of knowing the optimal action-value function does not hold in most cases. Therefore, we must either approximate the optimal action-value function or take other approaches in order to get deep reinforcement learning working.

## Sources:

- R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction [http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)
- D. Silver, “Lecture 1: Introduction to Reinforcement Learning”, [https://www.davidsilver.uk/teaching/](https://www.davidsilver.uk/teaching/)
- D. Silver, “Lecture 2: Markov Decision Processes”, [https://www.davidsilver.uk/teaching/](https://www.davidsilver.uk/teaching/)