<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tobirohrer.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tobirohrer.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-19T21:17:07+00:00</updated><id>https://tobirohrer.github.io/feed.xml</id><title type="html">Tobias Rohrer</title><subtitle>My homepage containing content about reinforcement learning and myself </subtitle><entry><title type="html">Reinforcement Learning Part 2 - Markov Decision Process (MDP)</title><link href="https://tobirohrer.github.io/blog/2022/rl-part2-mdp/" rel="alternate" type="text/html" title="Reinforcement Learning Part 2 - Markov Decision Process (MDP)"/><published>2022-10-13T06:00:00+00:00</published><updated>2022-10-13T06:00:00+00:00</updated><id>https://tobirohrer.github.io/blog/2022/rl-part2-mdp</id><content type="html" xml:base="https://tobirohrer.github.io/blog/2022/rl-part2-mdp/"><![CDATA[<p>Markov decision processes (MDPs) serve as a mathematical formalism to describe sequential decision making problems. It is used to formally define environments in reinforcement learning. The agent, which is the component that takes the decisions, interacts with this environment to learn a desired behavior in a trial and error manner. Note that the terms MDP and environment are often used interchangeably. Here, the term MDP is used when describing the formal properties, and the term environment when referring to implementation aspects of reinforcement learning.</p> <h1 id="elements-of-a-markov-decision-process">Elements of a Markov Decision Process</h1> <p>In the following, the components which are used to define the MDP are listed and described in more detail.</p> <h2 id="state-space">State Space</h2> <p>The <em>state space</em> defines the possible states that the MDP can possibly evolve into. A state is used to describe the MDP at a given time step. The agent uses a state as decision criterion for its choice of action (with the goal to maximize rewards).</p> <p>Let’s take chess as an example: One state would contain information about every single piece on the board. Based on this state, the agent is given the possibility to plan its next move (deciding which piece is moved to which field). This state could be either represented numerically or by an image of the chess board (which would make the overall task harder as it adds further complexity of image processing). In this example, the state space contains every possible (legal) positioning of chess pieces (according to the “Shannon number”, the state space of chess contains around \(10^{120}\) different states).</p> <h4 id="markov-property">Markov Property</h4> <p>A fundamental concept of states in MDPs is the <em>Markov property</em>, which defines that “The future is independent of the past given the present”, which can be formally described by:</p> \[P[S_{t+1}|S_t] = P[S_{t+1}|S_1, S_2..., S_t].\] <p>This means, in order to not violate the Markov Property, a state must “include information about all aspects of the past agent–environment interaction that make a difference for the future”(<cite>R. S. Sutton and A. G. Barto</cite>). In the end, the agent takes its decision based on the state of the current time step and not on states it has seen before (as if the agent had no memory).</p> <p>Let’s take table football as an example: Imagine the scenario where we want to train a table football goalkeeper to hold a ball. In this scenario, we define an image of the current game state as our state. Meaning, a state would look like this (Note that the ball is shown in orange):</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/markov_property-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/markov_property-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/markov_property-1400.webp"/> <img src="/assets/img/posts/markov_property.png" class="img-fluid center rounded z-depth-1" width="300" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Could you conclude from only this picture in which direction the goalkeeper should move? Probably not. The information about the direction and velocity of the ball is missing and can only be estimated from several consecutive images. This means, in order to follow the Markov property we must include those past states (which are images in our exemplary scenario) and therefore give the agent the possibility to reason about the direction and velocity of the ball.</p> <h4 id="start-and-terminal-states">Start and Terminal States</h4> <p>There are two types of special states. First, there are <em>initial states</em> which are often denoted as \(S_0\). Second, there are <em>terminal states</em> which mark the end of an episode. Terminal states are usually denoted with a capital T, like \(S_T\). An interaction between environment and agent that stretches from \(S_0, A_0, S_1, A_1..., S_T , A_T\) is called an <em>episode</em>. Note that not all MDPs have terminal states. Those that have terminal states are often referred to as <em>episodic MDPs</em>, whereas those without any terminal states are referred to as <em>continuing MDPs</em>.</p> <h2 id="action-space">Action Space</h2> <p>The <em>action space</em> defines the possible Actions \(A_t\) the agent can take in a given state. Taking an action triggers a state transition and results in a new MDP state \(S_{t+1}\). Action spaces can be categorized by their action type, which can be either discrete or continuous. In continuous action spaces, an action can take any real number in a specified interval. On the other hand, the actions in discrete action spaces can only take discrete values.</p> <p>Let’s take our table football example again where we want to train an agent to control the goalkeeper: When using a discrete action space, the available actions could be left, right, and “do nothing”. Left and right could move the goalkeeper to one of the sides by a fixed distance. In a continuous action space, the available actions would lie in an interval, which defines the distance and direction (e.g. negative: left, positive: right) the goalkeeper would move. In this example, choosing a continuous action space would offer greater control precision.</p> <h2 id="reward-function">Reward Function</h2> <p>After taking an action, the agent not only receives information about the new MDP state but also a scalar reward \(R_{t+1} \in \mathbb{R}\). The reward serves as feedback on the quality of a certain action in a given state. The goal of the agent is to maximize the sum rewards over time (as we will see in a minute, this sum gets <em>discounted</em>). The calculation of the reward can be expressed as a function \(r(s, a, s^\prime)\), which depends on the current state \(s\), the successor state \(s^\prime\) as well as the action \(a\) taken to transfer from the current to the successor state. The reward function is part of the implementation of every environment and is very important as it encodes the goal of the task at hand. Choosing the reward definition wisely is key to the functionality of reinforcement learning.</p> <p>The reward given to the agent can be categorized as either being <em>sparse</em> or <em>dense</em>. <em>Sparse rewards</em> are only given to the agent rarely (typically at the end of an episode). In contrast, <em>dense rewards</em> are given to the agent often (typically after every action). Let’s take our table football goalkeeper example again: In a <em>sparse reward</em> scenario, the agent would only receive a reward after the goalkeeper holds the ball (e.g. r=1) or a goal was scored (e.g. r=-1). In a <em>dense reward</em> scenario, the agent could receive a reward after every action in form of the distance between the goalkeeper and the ball (the lower the distance, the higher the reward). This way, the agent could easily learn the behavior of moving close to the ball in order to hold it. Most of the times, using a <em>dense reward</em> makes learning easier, as we give the agent feedback more often. On the other hand, sometimes <em>dense rewards</em> are hard to define (for example at chess). Lastly, by using <em>sparse rewards</em> we are indirectly providing prior knowledge to the agent (which we usually don´t want). Like in the goalkeeper scenario, we are providing the agent with the information that it is good to be close to the ball in order to hold it (wouldn´t it be nice, if the agent could pick this behavior up itself?).</p> <p>Another important concept related to the reward definition is <em>discounting</em>. With discounting we regulate the impact of time distant rewards. As an example: If we want to create a trading agent, we might want to define, that money now is worth more than money in two years. Therefore, we <em>discount</em> the rewards which might occur somewhere in the future.</p> <p>This concept is taken up by the <em>return</em> \(G_t\), which defines the sum of <em>discounted</em> future rewards from the current time step \(t\). It is defined as follows:</p> \[\begin{align} G_t &amp;= \sum\limits_{i=t+1}^\infty \gamma^{i-t+1} R_i \label{eq:return} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \\ &amp;= R_{t+1}+ \gamma G_{t+1}. &amp;&amp; \label{eq:return_recursive} \end{align}\] <p>The sum is discounted by factor \(\gamma \in [0,1]\) which is used to define how much impact future rewards have compared to immediate rewards. The higher \(\gamma\), the more impact future rewards have on the return. In the case where \(\gamma = 1\), rewards in the far future would have the same impact on the return as immediate rewards. If \(\gamma = 0\), we don’t care about any future rewards at all. The agent would learn a policy that only focuses on selecting an action that maximizes rewards in the present and ignores all future consequences this might cause (like eating a whole bag of gummy bears now because it is so delicious but completely ignoring the fact that this might cause stomach ache later). The right choice of \(\gamma\) depends on the problem at hand and is one of the most important hyperparameters in reinforcement learning.</p> <h2 id="transition-function">Transition Function</h2> <p>As mentioned before, taking an action causes the MDP to transfer into a new state. This transition can be described more formally by:</p> \[p(s^\prime, r|a,s) = P[S_{t+1}=s^\prime, R_t=r| A_{t}=a,S_{t}=s]\] <p>The transition function \(p\) defines the probability of arriving at the successor state \(s^\prime\) and get reward \(r\), if the agent chooses action \(a\) while being in state \(s\). Or to put it in other words: \(p\) defines the inner dynamics of the MDP.</p> <p>The transition function makes the environment either <em>stochastic</em> or <em>deterministic</em>. In <em>stochastic environments</em>, random dynamics are involved during state transitions. Meaning, given the same state \(s\) and action \(a\), the MDP can transition into a set of different possible successor states and experience a different reward.</p> <p>In contrast, in <em>deterministic environments</em>, no random dynamics are involved during state transition. This leads to the fact that the transition function exclusively takes the value 0 or 1. This means, when you take action \(a\) while being in state \(s\), you can always expect the same successor state \(s^\prime\) and \(reward\). </p> <h1 id="the-agent-side-of-a-mdp">The Agent Side of a MDP</h1> <p>The components described above defined the MDP, which is the formal definition of our environment in reinforcement learning. The other side of the equation is the agent, which takes decisions by choosing actions. Mostly we are interested in finding (or training) an agent that chooses (good) actions which maximize rewards. An agent chooses those actions based on a <em>policy</em>, which is denoted as \(\pi\). More formally, \(\pi\) specifies the behavior of an agent by defining a probability distribution of actions based on the current state:</p> \[\pi(a|s) = P[A_{t}=a| S_{t}=s].\] <p>When talking about <em>deep</em> reinforcement learning, the policy \(\pi\) is represented by a <em>deep</em> neural network.</p> <p>To be able to find policies that perform well, we need a way to evaluate them. Therefore, value functions are used in order to assess “how good it is for the agent to be in a given state”(<cite>R. S. Sutton and A. G. Barto</cite>). This is done by the so called <em>state-value function</em> \(V_{\pi}(s)\). It defines the expected future return when being in state \(s\) and following policy \(\pi\) from there on:</p> \[V_{\pi}(s) = E_{\pi}[G_{t} | S_{t} = s].\] <p>Derived from the state-value function, the <em>action-value</em> function \(Q_{\pi}(s, a)\) estimates the return when taking an action in a particular state and following policy \(\pi\) afterward:</p> \[Q_{\pi}(s, a) = E_{\pi}[G_{t} | S_{t} = s, A_{t} = a].\] <p>The action-values are commonly named <em>Q-values</em>. If assumed to act optimally after taking action \(a\), the action-value function is denoted as \(Q_{*}(s, a)\).</p> <p>In case the optimal action-value function \(Q_{*}(s, a)\) is known, acting optimally can be done by simply following a greedy policy, which chooses the action which maximizes \(Q_{*}(s, a)\) in each time step:</p> \[\pi^*= \underset{a}{\arg\max}\:Q_{*}(s,a).\] <p>Or in simple words, acting optimally can be done by choosing the best possible action in a given state. The best possible action is the one with the highest Q-value. Anyhow, the assumption of knowing the optimal action-value function does not hold in most cases. Therefore, we must either approximate the optimal action-value function or take other approaches in order to get deep reinforcement learning working.</p> <h2 id="sources">Sources:</h2> <ul> <li>R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a></li> <li>D. Silver, “Lecture 1: Introduction to Reinforcement Learning”, <a href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a></li> <li>D. Silver, “Lecture 2: Markov Decision Processes”, <a href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a></li> </ul>]]></content><author><name></name></author><category term="RL"/><category term="RL"/><summary type="html"><![CDATA[A detailed description of markov decision process which serves as mathematical foundation of formulating RL problems.]]></summary></entry><entry><title type="html">Reinforcement Learning Part 1 - Introduction</title><link href="https://tobirohrer.github.io/blog/2022/rl-part1-intro/" rel="alternate" type="text/html" title="Reinforcement Learning Part 1 - Introduction"/><published>2022-08-15T21:01:00+00:00</published><updated>2022-08-15T21:01:00+00:00</updated><id>https://tobirohrer.github.io/blog/2022/rl-part1-intro</id><content type="html" xml:base="https://tobirohrer.github.io/blog/2022/rl-part1-intro/"><![CDATA[<p>In reinforcement learning a so called <em>agent</em> learns to solve a task by learning from trial and error when interacting with an <em>environment</em>. This interaction is triggered by the agent by choosing an action \(A(s)\) (here, an action is denoted as function as the action to be chosen might depend on the current state). As a response to the action, the agent gets information about the new environment state \(S_{t+1}\) and a scalar reward \(R_{t+1}\). The following image shows this basic relationship between the agent and the environment:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/drl_basics-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/drl_basics-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/drl_basics-1400.webp"/> <img src="/assets/img/posts/drl_basics.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The reward serves as feedback on the quality of a certain action in a given state. The goal of the agent is to maximize the sum of rewards over time.</p> <h2 id="sources">Sources:</h2> <ul> <li>R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a></li> <li>D. Silver, “Lecture 1: Introduction to Reinforcement Learning”, <a href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a></li> </ul>]]></content><author><name></name></author><category term="RL"/><category term="RL"/><summary type="html"><![CDATA[A short intro into RL]]></summary></entry></feed>